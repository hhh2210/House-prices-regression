# LightGBM GPU è´å¶æ–¯ä¼˜åŒ–ä½¿ç”¨æŒ‡å—

## æ–°å¢åŠŸèƒ½

å·²æˆåŠŸé›†æˆ **Optuna** è´å¶æ–¯ä¼˜åŒ–æ¡†æ¶ï¼Œç›¸æ¯”åŸæœ‰çš„ RandomizedSearchCVï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

### Optuna ä¼˜åŠ¿
- ğŸš€ **æ›´æ™ºèƒ½çš„æœç´¢**ï¼šä½¿ç”¨ TPE (Tree-structured Parzen Estimator) ç®—æ³•ï¼Œè€Œééšæœºæœç´¢
- âœ‚ï¸ **è‡ªåŠ¨å‰ªæ**ï¼šé€šè¿‡ MedianPruner æå‰ç»ˆæ­¢è¡¨ç°å·®çš„è¯•éªŒï¼ŒèŠ‚çœæ—¶é—´
- ğŸ“Š **å®æ—¶è¿›åº¦**ï¼šæ˜¾ç¤ºæœ€ä½³å¾—åˆ†å’Œå®Œæˆ/å‰ªæè¯•éªŒç»Ÿè®¡
- ğŸ¯ **æ›´é«˜æ•ˆç‡**ï¼šé€šå¸¸æ¯”éšæœºæœç´¢å¿« 3-10 å€æ‰¾åˆ°æœ€ä¼˜å‚æ•°

## ä½¿ç”¨å‘½ä»¤

### 1. LightGBM + GPU + Optuna ä¼˜åŒ–ï¼ˆæ¨èï¼‰
```bash
# åŸºç¡€ç‰ˆæœ¬ - 50 æ¬¡è¯•éªŒï¼ˆé»˜è®¤ï¼‰
uv run python src/train_tree.py --model lgbm --gpu --tune

# æ·±åº¦æœç´¢ - 100 æ¬¡è¯•éªŒ
uv run python src/train_tree.py --model lgbm --gpu --tune --n_iter 100

# åŒ…å« AmesHousing æ•°æ®å¢å¼º
uv run python src/train_tree.py --model lgbm --gpu --tune  --include-ames --n_jobs 10
```

### 2. ä½¿ç”¨ä¼ ç»Ÿ RandomizedSearchCV
```bash
python src/train_tree.py --model lgbm --gpu --tune --tune-method random --n_iter 100
```

### 3. å…¶ä»–æ¨¡å‹æ”¯æŒ
```bash
# XGBoost + GPU
python src/train_tree.py --model xgb --gpu --tune --n_iter 50

# RandomForestï¼ˆæ—  GPU åŠ é€Ÿï¼‰
python src/train_tree.py --model rf --tune --n_iter 50

# HistGradientBoosting
python src/train_tree.py --model hgb --tune --n_iter 50
```

## å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--model` | `hgb` | æ¨¡å‹ç±»å‹ï¼š`lgbm`, `xgb`, `rf`, `hgb` |
| `--gpu` | `False` | å¯ç”¨ GPU åŠ é€Ÿï¼ˆä»… LightGBM/XGBoostï¼‰ |
| `--tune` | `False` | å¯ç”¨è¶…å‚æ•°ä¼˜åŒ– |
| `--tune-method` | `optuna` | ä¼˜åŒ–æ–¹æ³•ï¼š`optuna` æˆ– `random` |
| `--n_iter` | `50` | è¯•éªŒæ¬¡æ•°ï¼ˆoptunaï¼‰æˆ–è¿­ä»£æ¬¡æ•°ï¼ˆrandomï¼‰ |
| `--folds` | `5` | äº¤å‰éªŒè¯æŠ˜æ•° |
| `--include-ames` | `False` | ä½¿ç”¨ AmesHousing æ•°æ®å¢å¼º |

## LightGBM å‚æ•°æœç´¢ç©ºé—´

Optuna ä¼šåœ¨ä»¥ä¸‹èŒƒå›´å†…æ™ºèƒ½æœç´¢æœ€ä½³å‚æ•°ï¼š

```python
{
    "n_estimators": [1000, 5000],          # æ ‘çš„æ•°é‡
    "learning_rate": [0.01, 0.2],          # å­¦ä¹ ç‡ï¼ˆå¯¹æ•°åˆ†å¸ƒï¼‰
    "num_leaves": [15, 127],               # å¶å­èŠ‚ç‚¹æ•°
    "subsample": [0.5, 1.0],               # æ ·æœ¬é‡‡æ ·ç‡
    "colsample_bytree": [0.5, 1.0],        # ç‰¹å¾é‡‡æ ·ç‡
    "reg_lambda": [0.1, 2.0],              # L2 æ­£åˆ™åŒ–
    "min_child_samples": [5, 50],          # å¶å­æœ€å°æ ·æœ¬æ•°
}
```

## è¾“å‡ºç¤ºä¾‹

```
============================================================
Optuna Bayesian Optimization: 50 trials Ã— 5 folds
Using TPE sampler with median pruner
============================================================

Optuna Trials: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [15:32<00:00, best=0.1234]

Best CV RMSE(log): 0.123456
Best params: {'n_estimators': 2500, 'learning_rate': 0.0345, ...}

Optuna Statistics:
  Completed trials: 42
  Pruned trials: 8
```

## é¢„è®¡è€—æ—¶

ä½¿ç”¨ GPU åŠ é€Ÿçš„æƒ…å†µä¸‹ï¼š

| è¯•éªŒæ¬¡æ•° | é¢„è®¡è€—æ—¶ |
|----------|----------|
| 50 æ¬¡ | 10-20 åˆ†é’Ÿ |
| 100 æ¬¡ | 20-40 åˆ†é’Ÿ |
| 200 æ¬¡ | 40-80 åˆ†é’Ÿ |

*æ³¨ï¼šå‰ªææœºåˆ¶ä¼šæ˜¾è‘—å‡å°‘å®é™…è€—æ—¶ï¼Œè¡¨ç°å·®çš„è¯•éªŒä¼šæå‰ç»ˆæ­¢ã€‚*

## æœ€ä½³å®è·µ

1. **åˆæ¬¡æ¢ç´¢**ï¼šä½¿ç”¨ 50 æ¬¡è¯•éªŒå¿«é€Ÿæ‰¾åˆ°è¾ƒä¼˜åŒºåŸŸ
2. **ç²¾ç»†è°ƒä¼˜**ï¼šåœ¨å¥½çš„åŒºåŸŸåŸºç¡€ä¸Šè¿è¡Œ 100-200 æ¬¡è¯•éªŒ
3. **GPU åŠ é€Ÿ**ï¼šç¡®ä¿å®‰è£…äº†æ”¯æŒ GPU çš„ LightGBM ç‰ˆæœ¬
4. **æ•°æ®å¢å¼º**ï¼š`--include-ames` å¯èƒ½æå‡æ³›åŒ–èƒ½åŠ›

## æŠ€æœ¯ç»†èŠ‚

- **é‡‡æ ·å™¨**ï¼šTPESampler with seed for reproducibility
- **å‰ªæå™¨**ï¼šMedianPruner (n_startup_trials=5, n_warmup_steps=2)
- **ä¼˜åŒ–æ–¹å‘**ï¼šminimize RMSE(log)
- **CVç­–ç•¥**ï¼šKFold with shuffle

## ä¾èµ–é¡¹

```toml
optuna>=4.1.0
lightgbm>=4.6.0
xgboost  # å¯é€‰ï¼Œä»…ä½¿ç”¨ XGBoost æ—¶éœ€è¦
```
