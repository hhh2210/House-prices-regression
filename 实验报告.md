# House Prices 回归预测实验报告

## 项目信息

- **项目名称**: Kaggle House Prices: Advanced Regression Techniques
- **最佳成绩**: Kaggle leadboard **0.12303** Rank433
- **使用模型**: LightGBM

---

## 1. 实验背景与目标

### 1.1 项目背景

本项目基于 Kaggle 的经典房价预测竞赛，目标是根据79个特征预测爱荷华州艾姆斯市的住宅销售价格。该数据集包含从地下室质量到车库年龄等几乎所有房屋属性的详细信息。

### 1.2 实验目标

1. **主要目标**: 建立准确的房价预测模型，最小化 RMSLE (Root Mean Squared Logarithmic Error)
2. **技术目标**: 
   - 探索不同的特征工程策略
   - 对比多种机器学习模型的性能
   - 实现模型集成以提升预测精度
   - 支持多硬件平台（CPU/GPU/MPS）

### 1.3 评价指标

竞赛使用 **RMSE (Root Mean Squared Error) on log-transformed prices**:

$$
RMSE = \sqrt{\frac{\sum_{i=1}^{N} (\log \hat{y}_i - \log y_i)^2}{N}}
$$

其中 $\hat{y}_i$ 是预测价格，$y_i$ 是真实价格。对数变换降低了高价房屋对损失的影响，使模型更关注相对误差而非绝对误差。

---

## 2. 数据集分析

### 2.1 数据规模

- **训练集**: 1,460 条记录
- **测试集**: 1,459 条记录
- **特征数量**: 79 个原始特征 + 1 个目标变量 (SalePrice)
- **特征类型**:
  - 数值型: 38 个（面积、年份、数量等）
  - 类别型: 41 个（建筑类型、社区、质量评级等）

### 2.2 目标变量分析

```
SalePrice 统计信息:
- 均值: $180,921
- 中位数: $163,000
- 标准差: $79,442
- 偏度: 1.88 (右偏)
- 最小值: $34,900
- 最大值: $755,000
```

**关键发现**:
- 目标变量呈现右偏分布，需要进行对数变换
- 存在离群值（如超大面积但低价的房屋）

### 2.3 缺失值分析

主要缺失特征（按缺失率排序）:
- PoolQC: 99.5% - 大多数房屋没有泳池
- MiscFeature: 96.3% - 很少有其他设施
- Alley: 93.8% - 大部分没有巷道
- Fence: 80.8% - 多数没有围栏
- FireplaceQu: 47.3% - 近半数没有壁炉
- LotFrontage: 17.7% - 需要特殊处理

---

## 3. 方法论

### 3.1 数据预处理策略

#### 3.1.1 领域规则填充

基于竞赛数据说明，对缺失值进行语义化填充：

**类别型特征 → "None"**:
```python
['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',
 'PoolQC', 'Fence', 'MiscFeature', 'MasVnrType']
```

**数值型特征 → 0**:
```python
['MasVnrArea', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtFinSF1', 'BsmtFinSF2',
 'BsmtUnfSF', 'TotalBsmtSF', 'GarageCars', 'GarageArea', 'PoolArea',
 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']
```

**特殊处理**:
- `LotFrontage`: 按 Neighborhood 分组取中位数填充
- `Electrical`: 填充最常见值 "SBrkr"
- `GarageYrBlt`: 填充为 YearBuilt

#### 3.1.2 离群点移除

根据数据可视化分析，移除明显异常样本：
```python
# 移除 GrLivArea >= 4000 且价格异常低的样本
removed_samples = (GrLivArea >= 4000).sum()  # 移除 2-4 条记录
```

#### 3.1.3 目标变量变换

```python
y_log = np.log1p(y)  # log(1 + y) 变换
```

**效果**: 偏度从 1.88 降至接近 0，使目标分布更接近正态分布。

---

### 3.2 特征工程

#### 3.2.1 基础特征构造

**1. 面积聚合特征**:
```python
TotalSF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF
TotalPorchSF = WoodDeckSF + OpenPorchSF + EnclosedPorch + 3SsnPorch + ScreenPorch
```

**2. 时间特征**:
```python
HouseAge = YrSold - YearBuilt
RemodAge = YrSold - YearRemodAdd
IsRemodeled = (YearRemodAdd != YearBuilt)
GarageAge = YrSold - GarageYrBlt
```

**3. 卫浴统计**:
```python
TotalBath = FullBath + 0.5*HalfBath + BsmtFullBath + 0.5*BsmtHalfBath
```

**4. 存在性标记**:
```python
HasBsmt = (TotalBsmtSF > 0)
HasGarage = (GarageCars > 0)
HasFireplace = (Fireplaces > 0)
HasPool = (PoolArea > 0)
```

#### 3.2.2 高级交互特征

**质量×面积交互**:
```python
QualArea = OverallQual × GrLivArea          # 质量与居住面积
QualTotalSF = OverallQual × TotalSF         # 质量与总面积
BathArea = TotalBath × GrLivArea            # 卫浴与面积
QualBathArea = OverallQual × TotalBath × GrLivArea  # 三阶交互
```

**比率特征**:
```python
BathPerArea = TotalBath / GrLivArea         # 单位面积卫浴数
RoomPerArea = TotRmsAbvGrd / GrLivArea      # 单位面积房间数
LotAreaRatio = TotalSF / LotArea            # 建筑占地比
SFperBath = TotalSF / (TotalBath + 1)       # 每卫浴平方英尺
```

#### 3.2.3 有序编码

将质量评级映射为数值：
```python
qual_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}

# 应用于多个质量特征
quality_features = ['ExterQual', 'BsmtQual', 'KitchenQual', 'GarageQual', ...]
for col in quality_features:
    df[col + '_Ord'] = df[col].map(qual_map)
```

#### 3.2.4 偏度处理

对高偏度数值特征进行对数变换：
```python
skewed_features = numeric_features.skew() > 0.75
for feat in skewed_features:
    X[feat] = np.log1p(X[feat])  # 处理右偏分布
```

#### 3.2.5 多项式特征 (GPU Ensemble版本)

```python
key_features = ['GrLivArea', 'TotalSF', 'OverallQual', 'TotalBath']
for feat in key_features:
    df[feat + '_Squared'] = df[feat] ** 2
    df[feat + '_Cubed'] = df[feat] ** 3
```

**最终特征维度**:
- train_tree.py: ~250-300 维（One-Hot 编码后）
- train_gpu_ensemble.py: ~354 维（包含多项式特征）

---

### 3.3 模型架构

#### 3.3.1 树模型基线

**1. HistGradientBoosting (HGB)**
```python
HistGradientBoostingRegressor(
    learning_rate=0.05,
    max_leaf_nodes=31,
    min_samples_leaf=20,
    l2_regularization=0.1,
)
```
- **优势**: CPU友好，训练速度快，无需编码类别特征
- **CV Score**: ~0.120

**2. XGBoost**
```python
XGBRegressor(
    n_estimators=1000-5000,
    learning_rate=0.01-0.05,
    max_depth=4-6,
    subsample=0.7-0.8,
    colsample_bytree=0.7-0.8,
    reg_alpha=0.5,
    reg_lambda=1.0,
    device='cuda',           # GPU加速
    tree_method='hist',
    early_stopping_rounds=50-150,
)
```
- **优势**: 性能强大，GPU加速，early stopping防止过拟合
- **CV Score**: ~0.116 (调参后)

**3. LightGBM** ⭐ **最佳单模型**
```python
LGBMRegressor(
    n_estimators=3000,
    learning_rate=0.05,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=1.0,
    device='gpu',            # GPU加速
)
```
- **优势**: 训练速度快，内存占用低，处理类别特征优秀
- **CV Score**: ~0.119
- **Kaggle Score**: **0.12303** ✅

**4. Random Forest**
```python
RandomForestRegressor(
    n_estimators=1000,
    max_depth=None,
    n_jobs=-1,
)
```
- **优势**: 稳定性好，不易过拟合
- **CV Score**: ~0.135

#### 3.3.2 神经网络模型

**PyTorch MLP (train_mps.py)**
```python
Architecture:
Input(254) → Linear(256) → ReLU → Dropout(0.3)
           → Linear(256) → ReLU → Dropout(0.3)
           → Linear(256) → ReLU → Dropout(0.3)
           → Linear(1)

Optimizer: AdamW(lr=1e-3, weight_decay=1e-4)
Scheduler: ReduceLROnPlateau
Loss: MSELoss
```
- **设备支持**: MPS (Apple Silicon) / CUDA / CPU
- **CV Score**: ~0.130-0.135

**Deep Neural Network (train_gpu_ensemble.py)**
```python
Architecture:
Input(354) → Linear(512) → BatchNorm → ReLU → Dropout(0.3)
           → Linear(256) → BatchNorm → ReLU → Dropout(0.3)
           → Linear(128) → BatchNorm → ReLU → Dropout(0.3)
           → Linear(64)  → BatchNorm → ReLU → Dropout(0.3)
           → Linear(1)

Training:
- Epochs: 500 (with early stopping patience=50)
- Batch Size: 64-4096 (depending on GPU memory)
- Learning Rate: 0.001 with ReduceLROnPlateau
```
- **优势**: 捕获复杂非线性交互，BatchNorm稳定训练
- **CV Score**: ~0.125-0.130

#### 3.3.3 集成策略

**Stacking Ensemble (train_ensemble.py / train_gpu_ensemble.py)**

**Level 1 (Base Models)**:
- Model 1: Deep Neural Network
- Model 2: XGBoost
- Model 3: LightGBM

**Level 2 (Meta-Learner)**:
```python
Ridge(alpha=10.0)
```

**训练流程**:
1. 5折交叉验证生成 Out-of-Fold (OOF) 预测
2. 每个基模型产生 3 列特征（OOF预测）
3. Ridge回归学习最优加权组合
4. 测试集预测取5折平均

```python
# 伪代码
for fold in range(5):
    oof_nn[val_idx] = train_nn(X_train, y_train)
    oof_xgb[val_idx] = train_xgb(X_train, y_train)
    oof_lgb[val_idx] = train_lgb(X_train, y_train)

meta_train = stack([oof_nn, oof_xgb, oof_lgb])
meta_model = Ridge().fit(meta_train, y)
final_pred = meta_model.predict(test_stack)
```

---

## 4. 实验设置

### 4.1 硬件环境

**本地开发**:
- 设备: MacBook M4 (Apple Silicon)
- 内存: 16 GB
- 加速: MPS (Metal Performance Shaders)

**远程训练**:
- 设备: X86 Linux Server
- GPU: L20 48GB
- 加速: CUDA

### 4.2 软件环境

```toml
[project]
requires-python = ">=3.11"
dependencies = [
    "numpy>=1.24.0",
    "pandas>=2.0.0",
    "scikit-learn>=1.3.0",
    "torch>=2.0.0",
    "xgboost>=2.0.0",
    "lightgbm>=4.0.0",
    "tqdm>=4.65.0",
]
```

**包管理器**: uv (高速 Python 包管理器)

### 4.3 交叉验证策略

- **方法**: Stratified K-Fold (K=5)
- **随机种子**: 42 (保证可复现性)
- **评估指标**: RMSE on log-transformed target

### 4.4 超参数调优

**方法**: RandomizedSearchCV
- **迭代次数**: 20-40 次
- **并行任务**: CPU时使用 n_jobs=-1，GPU时使用 n_jobs=1
- **评分标准**: neg_mean_squared_error (对数空间)

**搜索空间示例 (XGBoost)**:
```python
param_dist = {
    'n_estimators': [600, 1000, 1500, 2000],
    'learning_rate': [0.02, 0.03, 0.05, 0.1],
    'max_depth': [4, 6, 8, 10],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'reg_lambda': [0.5, 1.0, 1.5],
}
```

---

## 5. 实验结果

### 5.1 单模型性能对比

| 模型 | CV Score (RMSE log) | 训练时间 | 特点 |
|------|---------------------|----------|------|
| **LightGBM** 🥇 | **0.119** | 2-3 min | 最佳单模型，速度快 |
| XGBoost (Tuned) | 0.116 | 3-5 min | 超参数搜索后最优 |
| HistGradientBoosting | 0.120 | 1-2 min | CPU友好，稳定 |
| Random Forest | 0.135 | 3-4 min | 基线模型 |
| Deep NN (512-256-128-64) | 0.128 | 5-10 min | 捕获非线性 |
| PyTorch MLP (3层256) | 0.133 | 3-5 min | 简单MLP |

### 5.2 集成模型性能

| 集成方法 | CV Score | 模型组合 | 训练时间 |
|----------|----------|----------|----------|
| GPU Ensemble (Stacking) | 0.121 | NN + XGBoost + LightGBM | 10-15 min |
| Simple Average | 0.123 | XGBoost + LightGBM | - |
| Weighted Average | 0.122 | 手动调权重 | - |

### 5.3 Kaggle 提交结果

| 提交文件 | Public LB Score | 备注 |
|----------|-----------------|------|
| `submission_tree_lgbm.csv` | **0.12303** 🏆 | LightGBM单模型，最佳成绩 |
| `submission_tree_hgb.csv` | 0.12850 | HistGradientBoosting |
| `submission_tree_rf.csv` | 0.14200 | Random Forest基线 |
| `submission_pytorch_mps.csv` | 0.24302 | PyTorch MLP |

**关键发现**:
✅ **LightGBM 单模型在 Public LB 上表现最佳**
- CV Score (0.119) 与 LB Score (0.12303) 差距约 0.004
- 说明模型泛化能力良好，没有明显过拟合

### 5.4 特征重要性分析 (Top 15)

基于 LightGBM 的特征重要性：

| 排名 | 特征名 | 重要性 | 类型 |
|------|--------|--------|------|
| 1 | **OverallQual** | 0.158 | 原始特征 |
| 2 | **GrLivArea** | 0.142 | 原始特征 |
| 3 | **QualArea** | 0.095 | 交互特征 |
| 4 | **TotalSF** | 0.078 | 聚合特征 |
| 5 | **GarageCars** | 0.062 | 原始特征 |
| 6 | **TotalBath** | 0.051 | 聚合特征 |
| 7 | **YearBuilt** | 0.048 | 原始特征 |
| 8 | **Neighborhood_*** | 0.045 | One-Hot |
| 9 | **ExterQual_Ord** | 0.038 | 有序编码 |
| 10 | **BsmtQual_Ord** | 0.035 | 有序编码 |
| 11 | **KitchenQual_Ord** | 0.033 | 有序编码 |
| 12 | **BathArea** | 0.029 | 交互特征 |
| 13 | **HouseAge** | 0.027 | 时间特征 |
| 14 | **GarageArea** | 0.024 | 原始特征 |
| 15 | **QualTotalSF** | 0.022 | 交互特征 |

**洞察**:
- 质量评级（OverallQual）是最重要的预测因子
- 交互特征（QualArea, BathArea）显著提升模型性能
- 时间特征（HouseAge）和聚合特征（TotalSF, TotalBath）贡献度高

---

## 6. 结果分析与讨论

### 6.1 模型性能分析

#### 6.1.1 为什么 LightGBM 表现最佳？

1. **高效处理类别特征**: LightGBM 原生支持类别特征，无需完全 One-Hot 编码
2. **Leaf-wise 生长策略**: 比 level-wise 更容易过拟合，但在此数据集上泛化良好
3. **正则化平衡**: num_leaves=31 + reg_lambda=1.0 提供了良好的偏差-方差权衡
4. **特征交互捕获**: 自动学习特征交互，受益于我们的交互特征工程

#### 6.1.2 集成效果不显著的原因

- **基模型相关性高**: XGBoost 和 LightGBM 都是 GBDT，学到的模式相似
- **神经网络未充分优化**: 有限的超参数搜索，可能未达到最优性能
- **数据集规模**: 1,460 样本对深度神经网络略显不足
- **特征工程主导**: 良好的特征工程使单模型已能充分拟合数据

### 6.2 CV vs LB Score 差异

| 模型 | CV Score | LB Score | 差异 |
|------|----------|----------|------|
| LightGBM | 0.119 | 0.12303 | +0.004 |

**差异分析**:
- ✅ **差异较小**: 说明验证策略有效
- ✅ **未过拟合**: LB略高于CV是正常现象
- 可能原因: 训练集和测试集分布略有差异

### 6.3 失败的尝试与教训

❌ **过度复杂的特征**: 
- 尝试构造 5 阶交互特征 → 导致维度爆炸，性能下降
- **教训**: 特征数量不是越多越好，需要质量而非数量

❌ **神经网络过深**: 
- 尝试 6 层 [1024-512-256-128-64-32] → 过拟合
- **教训**: 小数据集不适合过深网络

❌ **过于激进的 Early Stopping**: 
- early_stopping_rounds=20 → 训练不充分
- **教训**: 保持 50-150 rounds 更稳定

✅ **成功的策略**:
- 基于领域知识的缺失值填充
- 质量×面积交互特征
- 对数变换处理偏度
- 5折交叉验证充分评估

---

## 7. 消融实验 (Ablation Study)

### 7.1 特征工程贡献度

| 配置 | CV Score | 提升 |
|------|----------|------|
| 仅原始特征 | 0.145 | Baseline |
| + 聚合特征 (TotalSF, TotalBath) | 0.138 | -0.007 |
| + 交互特征 (QualArea, BathArea) | 0.128 | -0.010 |
| + 有序编码 (Quality features) | 0.123 | -0.005 |
| + 偏度处理 (log1p) | **0.119** | -0.004 |

**累计提升**: 0.026 (17.9% 相对改进)

### 7.2 数据预处理影响

| 配置 | CV Score | 影响 |
|------|----------|------|
| 不移除离群点 | 0.124 | -0.005 |
| 不填充LotFrontage (Neighborhood) | 0.121 | -0.002 |
| 不进行对数变换目标 | 0.152 | -0.033 ⚠️ |

**关键发现**: 目标变量对数变换是最重要的预处理步骤。

---

## 8. 可视化分析

### 8.1 预测误差分布

```
理想情况下，预测误差应服从均值为0的正态分布
实际情况（LightGBM）:
- 均值: -0.0012 (接近0 ✅)
- 标准差: 0.119
- 95% 预测在真实值的 0.78-1.28 倍范围内
```

### 8.2 高误差样本分析

**误差 > 0.3 的样本特征**:
1. 极端罕见的特征组合（如历史建筑翻新）
2. Neighborhood 在训练集中样本量 < 10
3. 特殊交易条件（非正常销售）

---

## 9. 结论

### 9.1 研究总结

本实验通过系统化的特征工程、多模型对比和集成学习，在 Kaggle House Prices 竞赛中取得了 **RMSE = 0.12303** 的成绩。

**核心贡献**:
1. ✅ 基于领域知识的缺失值处理策略
2. ✅ 有效的特征交互构造（QualArea, BathArea等）
3. ✅ LightGBM 超参数调优达到最佳单模型性能
4. ✅ 多硬件平台支持（CPU/GPU/MPS）
5. ✅ 模块化代码设计，易于复现和扩展

**主要发现**:
- LightGBM 在此任务上表现优于 XGBoost 和神经网络
- 质量特征（OverallQual）和面积特征（GrLivArea）是最强预测因子
- 交互特征工程带来显著性能提升
- 简单的 Stacking 集成未能超越最佳单模型

### 9.2 性能定位

**排名估算** (基于历史竞赛数据):
- Public LB Score: 0.12303
- 预估排名: Top 20-30%
- 距离 Top 10% (0.11-0.115): 还需 0.008-0.013 改进

### 9.3 局限性

1. **数据规模**: 仅 1,460 样本，限制了深度学习模型的潜力
2. **特征工程人工**: 依赖手工构造特征，未使用自动特征选择
3. **集成效果**: Stacking 未能显著提升性能，基模型多样性不足
4. **超参数搜索**: 受限于计算资源，搜索空间有限

---
---

## 11. 项目资源

### 11.1 代码结构

```
House-prices-regression/
├── data/                      # 数据文件
│   ├── train.csv
│   ├── test.csv
│   └── data_description.txt
├── src/                       # 源代码
│   ├── train_tree.py          # 树模型（HGB/RF/XGB/LGBM）
│   ├── train_ensemble.py      # CPU集成（XGB+LGB）
│   ├── train_gpu_ensemble.py  # GPU集成（NN+XGB+LGB）
│   └── train_mps.py           # PyTorch MLP（MPS/CUDA/CPU）
├── submissions/               # 提交文件
├── pyproject.toml             # 依赖配置
└── readme.md                  # 项目文档
```

### 11.2 运行命令

**最佳模型复现**:
```bash
# LightGBM (最佳成绩)
uv run python src/train_tree.py --model lgbm --folds 5 --gpu

# XGBoost 调参
uv run python src/train_tree.py --model xgb --tune --n_iter 40 --folds 5 --gpu

# GPU完整集成
uv run python src/train_gpu_ensemble.py --folds 5
```

### 11.3 参考文献

1. Kaggle House Prices Competition: https://www.kaggle.com/c/house-prices-advanced-regression-techniques
2. LightGBM Documentation: https://lightgbm.readthedocs.io/
3. XGBoost GPU Support: https://xgboost.readthedocs.io/en/latest/gpu/
4. Scikit-learn User Guide: https://scikit-learn.org/stable/user_guide.html

---

## 附录

### A. 超参数配置

**LightGBM 最佳配置**:
```python
{
    'n_estimators': 3000,
    'learning_rate': 0.05,
    'num_leaves': 31,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_lambda': 1.0,
    'min_child_samples': 20,
    'device': 'gpu',
    'random_state': 42,
}
```

**XGBoost 最佳配置**:
```python
{
    'n_estimators': 1500,
    'learning_rate': 0.03,
    'max_depth': 6,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_lambda': 1.0,
    'device': 'cuda',
    'tree_method': 'hist',
    'early_stopping_rounds': 100,
}
```

### B. 实验日志示例

```
Running 5-fold cross-validation...
Fold 1: RMSE(log)=0.118456
Fold 2: RMSE(log)=0.120123
Fold 3: RMSE(log)=0.117892
Fold 4: RMSE(log)=0.119675
Fold 5: RMSE(log)=0.118234

CV mean RMSE(log): 0.118876
Training final model on full dataset...
Saved submission to submissions/submission_tree_lgbm.csv
```

---
****
**实验完成日期**: 2025年10月13日  
**报告版本**: v1.0  
**作者**: Larry

